---
title: 单个 6.20TB 的超大 csv 文件保持顺序的情况下进行去除重

description: how to remove duplicates in large file
date: 2024-05-11T07:07:07+01:00

categories:
 - tool
 - multi thread
 - file
 
tags:
 - multi thread
 - file
---

# 单个 6.20TB 的超大 csv 文件保持顺序的情况下进行去除重

原帖在[这里](https://v2ex.com/t/1046023#reply95).具体的需求如下:
- 行数是 203 亿，平均行长 335
- 去重是基于整行文本
- 前缀重复度不高，没有 ID
- 内存最大256GB
- 重点是文件保持顺序

本文只讨论几种可行性的方案。
## 中间件方案(基于硬盘)
通过中间件来处理是可行的，无论你是mysql、postgresql、sqlite、hbase还是kvrocks，都可以在插入的时候判断同样的内容是否存在。而这些数据库是基于磁盘的，因此磁盘的IO决定了这个方案的上限。

以kvrocks为例(我前段时间刚好压测过kvrocks)，kvrocks(部署在SSD上)单机指令的TPS为10万。那么处理203亿行数据的时间为**2.3天**。我们可以部署kvrocks集群来增加kvrocks的吞吐量，由于本次需求只限制了内存，没有限制其他的CPU等，我们可以尽量多部署几个节点，让kvrocks集群的吞吐量高于我们读文件的性能即可。我在我本机SSD上测试的读取文件的性能为1秒钟150万行，那我们部署15个kvrocks节点即可。

### 优点
这个方案的好处在于只需要很小的**内存**。中间件简单，只要部署一下kvrocks集群就行了。受限于写文件性能，因此没有加行号重新写入文件。所以本方案还是推荐**单线程**读文件,最后处理完的结果就是最终的结果(不需要重新排序)。


## 中间件方案(基于内存)
使用redis的**布隆过滤器**能够很好的处理重复的数据。使用redis的限制是内存，我们通过[这个网站](https://krisives.github.io/bloom-calculator/)来计算一下所需要的内存。

- 203亿个key，允许错误率1%时，需要24GB内存
- 203亿个key，允许错误率0.1%时,需要35GB内存

看样子内存是符合要求了。我们再计算一下所用时间。


单机redis的TPS为20W,那么处理203亿行数据的时间为**1天**。

集群redis所用的时间和redis的节点数有关系，集群节点数越多,则TPS越高。由于最大内存为256GB，CPU没有限制,那么我们可以部署9个redis主节点，总共消耗24GBb*9=216Gb。则**理想状态下**处理203亿行数据所用时间为**3小时**。

什么是理想状态？数据完全离散，每行数据都落到不同的分区。这个视具体的数据情况而定。

第二个问题是当我们部署redis集群后，redis集群的吞吐量为180万每秒,而我们使用单线程读取文件，能达到这个量级吗？我试了一下流式读取SSD上的文件,每秒钟大概读150万的样子。如此看来读文件也不是瓶颈，而且我们还优化到了3小时。

### 优点
这个方案的好处在于中间件简单，只要部署一下redis集群就行了。由于是单线程读文件然后处理，因此也不需要重排序。

**使用布隆过滤器会有百分比的误差，几百亿的数据计算一下，结果要丢失几千万的数据，这个成本太大。**
## 分治法
我当初看到这个需求的时候第一时间想要的也是分治法。我们就按照分治法的思路分析一下可行性。
1. hash
2. 加序号
3. 按照hash分区
4. 逐个分区处理
5. 分区内排序

由于分治法会把整个文件通过hash算法重新分散到不同的文件中，对于文中的需求**按照文件顺序**，则需要添加行号来用于最后的排序。第1，2，3步都是用单线程进行操作。在我本机SSD上测试了一下，处理1000万条数据的时间为2分钟，大量的时间都花在写文件上。以同样的性能评估203亿行的数据执行1,2,3的步骤，则所花费的时间为**2.9天**。

后续的处理不在赘述，因为hash分区重新写文件的时间太久已经明显的不如其他的方案。



### Spark
答主没有用过Spark，不知道具体的分区消耗多少内存以及读取性能和处理性能，无法给出具体的可行性能答案。
